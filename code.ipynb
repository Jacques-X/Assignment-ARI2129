{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c9b209",
   "metadata": {},
   "source": [
    "# 2. Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53cc4e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames had already been extracted, skipping...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_dir, frame_rate=1):\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not video.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps\n",
    "    \n",
    "    print(f\"Video FPS: {fps}\")\n",
    "    print(f\"Total frames: {total_frames}\")\n",
    "    print(f\"Duration: {duration:.2f} seconds\")\n",
    "    \n",
    "    # Initialize frame counter\n",
    "    count = 0\n",
    "    frame_count = 0\n",
    "    \n",
    "    # Read until video is completed\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        \n",
    "        # Break the loop if we have reached the end of the video\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Save frame at the specified interval\n",
    "        if count % 1 == 0:\n",
    "            frame_filename = os.path.join(output_dir, f\"frame_{frame_count:04d}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            print(f\"Extracted: {frame_filename}\")\n",
    "            frame_count += 1\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    # Release the video capture object\n",
    "    video.release()\n",
    "    \n",
    "    print(f\"Extraction complete. {frame_count} frames extracted to {output_dir}\")\n",
    "\n",
    "def get_file_paths():\n",
    "    file_paths = []\n",
    "\n",
    "    for root, dirs, files in os.walk(\"videos\"):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "if not os.path.isdir('frames'): # If it exists, it was done already so do not run\n",
    "    file_paths = get_file_paths()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        extract_frames(file_path, os.path.join(\"frames\", os.path.basename(file_path)))\n",
    "else :\n",
    "    print(\"Frames had already been extracted, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6606cc",
   "metadata": {},
   "source": [
    "# 3. Feature Detection and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4367854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - - ORB - -\n",
      "Vid1.mp4/frame_0000.jpg: 976 keypoints in 0.0046s\n",
      "Vid1.mp4/frame_0001.jpg: 977 keypoints in 0.0047s\n",
      "Vid1.mp4/frame_0002.jpg: 976 keypoints in 0.0048s\n",
      "Vid1.mp4/frame_0003.jpg: 976 keypoints in 0.0044s\n",
      "Vid1.mp4/frame_0004.jpg: 977 keypoints in 0.0045s\n",
      "Vid1.mp4/frame_0005.jpg: 983 keypoints in 0.0047s\n",
      "Vid1.mp4/frame_0006.jpg: 985 keypoints in 0.0051s\n",
      "Vid1.mp4/frame_0007.jpg: 985 keypoints in 0.0046s\n",
      "Vid1.mp4/frame_0008.jpg: 980 keypoints in 0.0045s\n",
      "Vid1.mp4/frame_0009.jpg: 980 keypoints in 0.0046s\n",
      "Vid2.mp4/frame_0000.jpg: 947 keypoints in 0.0046s\n",
      "Vid2.mp4/frame_0001.jpg: 954 keypoints in 0.0041s\n",
      "Vid2.mp4/frame_0002.jpg: 948 keypoints in 0.0045s\n",
      "Vid2.mp4/frame_0003.jpg: 957 keypoints in 0.0046s\n",
      "Vid2.mp4/frame_0004.jpg: 952 keypoints in 0.0041s\n",
      "Vid2.mp4/frame_0005.jpg: 960 keypoints in 0.0041s\n",
      "Vid2.mp4/frame_0006.jpg: 951 keypoints in 0.0042s\n",
      "Vid2.mp4/frame_0007.jpg: 946 keypoints in 0.0044s\n",
      "Vid2.mp4/frame_0008.jpg: 949 keypoints in 0.0041s\n",
      "Vid2.mp4/frame_0009.jpg: 953 keypoints in 0.0041s\n",
      "Vid3.mp4/frame_0000.jpg: 944 keypoints in 0.0046s\n",
      "Vid3.mp4/frame_0001.jpg: 941 keypoints in 0.0046s\n",
      "Vid3.mp4/frame_0002.jpg: 940 keypoints in 0.0043s\n",
      "Vid3.mp4/frame_0003.jpg: 945 keypoints in 0.0043s\n",
      "Vid3.mp4/frame_0004.jpg: 951 keypoints in 0.0048s\n",
      "Vid3.mp4/frame_0005.jpg: 951 keypoints in 0.0043s\n",
      "Vid3.mp4/frame_0006.jpg: 956 keypoints in 0.0051s\n",
      "Vid3.mp4/frame_0007.jpg: 948 keypoints in 0.0043s\n",
      "Vid3.mp4/frame_0008.jpg: 950 keypoints in 0.0044s\n",
      "Vid3.mp4/frame_0009.jpg: 948 keypoints in 0.0043s\n",
      "Vid4.mp4/frame_0000.jpg: 1000 keypoints in 0.0047s\n",
      "Vid4.mp4/frame_0001.jpg: 994 keypoints in 0.0047s\n",
      "Vid4.mp4/frame_0002.jpg: 995 keypoints in 0.0046s\n",
      "Vid4.mp4/frame_0003.jpg: 995 keypoints in 0.0049s\n",
      "Vid4.mp4/frame_0004.jpg: 1000 keypoints in 0.0046s\n",
      "Vid4.mp4/frame_0005.jpg: 1000 keypoints in 0.0047s\n",
      "Vid4.mp4/frame_0006.jpg: 1000 keypoints in 0.0049s\n",
      "Vid4.mp4/frame_0007.jpg: 1000 keypoints in 0.0050s\n",
      "Vid4.mp4/frame_0008.jpg: 994 keypoints in 0.0051s\n",
      "Vid4.mp4/frame_0009.jpg: 995 keypoints in 0.0046s\n",
      "Vid5.mp4/frame_0000.jpg: 978 keypoints in 0.0045s\n",
      "Vid5.mp4/frame_0001.jpg: 975 keypoints in 0.0046s\n",
      "Vid5.mp4/frame_0002.jpg: 972 keypoints in 0.0049s\n",
      "Vid5.mp4/frame_0003.jpg: 976 keypoints in 0.0046s\n",
      "Vid5.mp4/frame_0004.jpg: 979 keypoints in 0.0044s\n",
      "Vid5.mp4/frame_0005.jpg: 978 keypoints in 0.0045s\n",
      "Vid5.mp4/frame_0006.jpg: 986 keypoints in 0.0045s\n",
      "Vid5.mp4/frame_0007.jpg: 983 keypoints in 0.0044s\n",
      "Vid5.mp4/frame_0008.jpg: 976 keypoints in 0.0047s\n",
      "Vid5.mp4/frame_0009.jpg: 984 keypoints in 0.0052s\n",
      "Average keypoints: 970.92 | Average Time: 0.00s\n",
      "\n",
      " - - SIFT - -\n",
      "Vid1.mp4/frame_0000.jpg: 312 keypoints in 0.0431s\n",
      "Vid1.mp4/frame_0001.jpg: 321 keypoints in 0.0374s\n",
      "Vid1.mp4/frame_0002.jpg: 337 keypoints in 0.0349s\n",
      "Vid1.mp4/frame_0003.jpg: 363 keypoints in 0.0410s\n",
      "Vid1.mp4/frame_0004.jpg: 368 keypoints in 0.0361s\n",
      "Vid1.mp4/frame_0005.jpg: 381 keypoints in 0.0400s\n",
      "Vid1.mp4/frame_0006.jpg: 382 keypoints in 0.0392s\n",
      "Vid1.mp4/frame_0007.jpg: 334 keypoints in 0.0326s\n",
      "Vid1.mp4/frame_0008.jpg: 354 keypoints in 0.0363s\n",
      "Vid1.mp4/frame_0009.jpg: 369 keypoints in 0.0802s\n",
      "Vid2.mp4/frame_0000.jpg: 214 keypoints in 0.0323s\n",
      "Vid2.mp4/frame_0001.jpg: 217 keypoints in 0.0324s\n",
      "Vid2.mp4/frame_0002.jpg: 219 keypoints in 0.0387s\n",
      "Vid2.mp4/frame_0003.jpg: 221 keypoints in 0.0422s\n",
      "Vid2.mp4/frame_0004.jpg: 214 keypoints in 0.0390s\n",
      "Vid2.mp4/frame_0005.jpg: 199 keypoints in 0.0349s\n",
      "Vid2.mp4/frame_0006.jpg: 239 keypoints in 0.0346s\n",
      "Vid2.mp4/frame_0007.jpg: 214 keypoints in 0.0315s\n",
      "Vid2.mp4/frame_0008.jpg: 210 keypoints in 0.0295s\n",
      "Vid2.mp4/frame_0009.jpg: 203 keypoints in 0.0346s\n",
      "Vid3.mp4/frame_0000.jpg: 306 keypoints in 0.0361s\n",
      "Vid3.mp4/frame_0001.jpg: 336 keypoints in 0.0345s\n",
      "Vid3.mp4/frame_0002.jpg: 357 keypoints in 0.0356s\n",
      "Vid3.mp4/frame_0003.jpg: 359 keypoints in 0.0332s\n",
      "Vid3.mp4/frame_0004.jpg: 368 keypoints in 0.0344s\n",
      "Vid3.mp4/frame_0005.jpg: 361 keypoints in 0.0313s\n",
      "Vid3.mp4/frame_0006.jpg: 361 keypoints in 0.0314s\n",
      "Vid3.mp4/frame_0007.jpg: 360 keypoints in 0.0314s\n",
      "Vid3.mp4/frame_0008.jpg: 372 keypoints in 0.0326s\n",
      "Vid3.mp4/frame_0009.jpg: 339 keypoints in 0.0310s\n",
      "Vid4.mp4/frame_0000.jpg: 662 keypoints in 0.0323s\n",
      "Vid4.mp4/frame_0001.jpg: 674 keypoints in 0.0338s\n",
      "Vid4.mp4/frame_0002.jpg: 688 keypoints in 0.0345s\n",
      "Vid4.mp4/frame_0003.jpg: 673 keypoints in 0.0334s\n",
      "Vid4.mp4/frame_0004.jpg: 728 keypoints in 0.0326s\n",
      "Vid4.mp4/frame_0005.jpg: 719 keypoints in 0.0341s\n",
      "Vid4.mp4/frame_0006.jpg: 665 keypoints in 0.0315s\n",
      "Vid4.mp4/frame_0007.jpg: 686 keypoints in 0.0326s\n",
      "Vid4.mp4/frame_0008.jpg: 688 keypoints in 0.0358s\n",
      "Vid4.mp4/frame_0009.jpg: 723 keypoints in 0.0340s\n",
      "Vid5.mp4/frame_0000.jpg: 622 keypoints in 0.0327s\n",
      "Vid5.mp4/frame_0001.jpg: 657 keypoints in 0.0328s\n",
      "Vid5.mp4/frame_0002.jpg: 632 keypoints in 0.0320s\n",
      "Vid5.mp4/frame_0003.jpg: 666 keypoints in 0.0340s\n",
      "Vid5.mp4/frame_0004.jpg: 648 keypoints in 0.0319s\n",
      "Vid5.mp4/frame_0005.jpg: 654 keypoints in 0.0330s\n",
      "Vid5.mp4/frame_0006.jpg: 640 keypoints in 0.0352s\n",
      "Vid5.mp4/frame_0007.jpg: 634 keypoints in 0.0342s\n",
      "Vid5.mp4/frame_0008.jpg: 628 keypoints in 0.0431s\n",
      "Vid5.mp4/frame_0009.jpg: 642 keypoints in 0.0374s\n",
      "Average keypoints: 450.38 | Average Time: 0.04s\n",
      "\n",
      " - - BRISK - -\n",
      "Vid1.mp4/frame_0000.jpg: 355 keypoints in 0.0129s\n",
      "Vid1.mp4/frame_0001.jpg: 347 keypoints in 0.0120s\n",
      "Vid1.mp4/frame_0002.jpg: 333 keypoints in 0.0120s\n",
      "Vid1.mp4/frame_0003.jpg: 363 keypoints in 0.0121s\n",
      "Vid1.mp4/frame_0004.jpg: 362 keypoints in 0.0118s\n",
      "Vid1.mp4/frame_0005.jpg: 366 keypoints in 0.0123s\n",
      "Vid1.mp4/frame_0006.jpg: 369 keypoints in 0.0124s\n",
      "Vid1.mp4/frame_0007.jpg: 374 keypoints in 0.0125s\n",
      "Vid1.mp4/frame_0008.jpg: 386 keypoints in 0.0121s\n",
      "Vid1.mp4/frame_0009.jpg: 381 keypoints in 0.0131s\n",
      "Vid2.mp4/frame_0000.jpg: 422 keypoints in 0.0146s\n",
      "Vid2.mp4/frame_0001.jpg: 387 keypoints in 0.0124s\n",
      "Vid2.mp4/frame_0002.jpg: 356 keypoints in 0.0121s\n",
      "Vid2.mp4/frame_0003.jpg: 387 keypoints in 0.0126s\n",
      "Vid2.mp4/frame_0004.jpg: 381 keypoints in 0.0123s\n",
      "Vid2.mp4/frame_0005.jpg: 379 keypoints in 0.0123s\n",
      "Vid2.mp4/frame_0006.jpg: 375 keypoints in 0.0146s\n",
      "Vid2.mp4/frame_0007.jpg: 397 keypoints in 0.0118s\n",
      "Vid2.mp4/frame_0008.jpg: 414 keypoints in 0.0132s\n",
      "Vid2.mp4/frame_0009.jpg: 374 keypoints in 0.0131s\n",
      "Vid3.mp4/frame_0000.jpg: 469 keypoints in 0.0145s\n",
      "Vid3.mp4/frame_0001.jpg: 436 keypoints in 0.0139s\n",
      "Vid3.mp4/frame_0002.jpg: 414 keypoints in 0.0125s\n",
      "Vid3.mp4/frame_0003.jpg: 446 keypoints in 0.0143s\n",
      "Vid3.mp4/frame_0004.jpg: 439 keypoints in 0.0135s\n",
      "Vid3.mp4/frame_0005.jpg: 457 keypoints in 0.0135s\n",
      "Vid3.mp4/frame_0006.jpg: 461 keypoints in 0.0134s\n",
      "Vid3.mp4/frame_0007.jpg: 459 keypoints in 0.0137s\n",
      "Vid3.mp4/frame_0008.jpg: 467 keypoints in 0.0140s\n",
      "Vid3.mp4/frame_0009.jpg: 480 keypoints in 0.0136s\n",
      "Vid4.mp4/frame_0000.jpg: 829 keypoints in 0.0173s\n",
      "Vid4.mp4/frame_0001.jpg: 860 keypoints in 0.0174s\n",
      "Vid4.mp4/frame_0002.jpg: 787 keypoints in 0.0168s\n",
      "Vid4.mp4/frame_0003.jpg: 826 keypoints in 0.0178s\n",
      "Vid4.mp4/frame_0004.jpg: 830 keypoints in 0.0175s\n",
      "Vid4.mp4/frame_0005.jpg: 856 keypoints in 0.0179s\n",
      "Vid4.mp4/frame_0006.jpg: 862 keypoints in 0.0188s\n",
      "Vid4.mp4/frame_0007.jpg: 860 keypoints in 0.0182s\n",
      "Vid4.mp4/frame_0008.jpg: 853 keypoints in 0.0175s\n",
      "Vid4.mp4/frame_0009.jpg: 858 keypoints in 0.0188s\n",
      "Vid5.mp4/frame_0000.jpg: 650 keypoints in 0.0156s\n",
      "Vid5.mp4/frame_0001.jpg: 584 keypoints in 0.0147s\n",
      "Vid5.mp4/frame_0002.jpg: 588 keypoints in 0.0146s\n",
      "Vid5.mp4/frame_0003.jpg: 593 keypoints in 0.0145s\n",
      "Vid5.mp4/frame_0004.jpg: 618 keypoints in 0.0152s\n",
      "Vid5.mp4/frame_0005.jpg: 600 keypoints in 0.0154s\n",
      "Vid5.mp4/frame_0006.jpg: 650 keypoints in 0.0152s\n",
      "Vid5.mp4/frame_0007.jpg: 620 keypoints in 0.0158s\n",
      "Vid5.mp4/frame_0008.jpg: 610 keypoints in 0.0150s\n",
      "Vid5.mp4/frame_0009.jpg: 625 keypoints in 0.0160s\n",
      "Average keypoints: 531.90 | Average Time: 0.01s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#feature detection using ORB, SIFT, and BRISK\n",
    "# Keeping the original function signatures as requested\n",
    "def load_images_from_all_videos(base_folder, max_images_per_video=10):\n",
    "    \"\"\"Loads images from all video subfolders (limited per folder).\"\"\"\n",
    "    # This function will still load from all subfolders for compatibility,\n",
    "    # but the processing loop below will handle per-video logic.\n",
    "    all_images = []\n",
    "    video_folders = sorted(os.listdir(base_folder))\n",
    "\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "\n",
    "    for video_folder in video_folders:\n",
    "        full_path = os.path.join(base_folder, video_folder)\n",
    "        if not os.path.isdir(full_path):\n",
    "            continue  # skip non-folder entries\n",
    "\n",
    "        image_filenames = sorted(os.listdir(full_path))\n",
    "        image_files = [f for f in image_filenames if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "\n",
    "        for filename in image_files[:max_images_per_video]:\n",
    "            img_path = os.path.join(full_path, filename)\n",
    "            img_color = cv2.imread(img_path)\n",
    "            if img_color is None:\n",
    "                print(f\"Warning: Couldn't read {img_path}\")\n",
    "                continue\n",
    "            img_gray = cv2.cvtColor(img_color, cv2.COLOR_BGR2GRAY)\n",
    "            # Store video_folder name with the image data\n",
    "            all_images.append((video_folder, filename, img_color, img_gray))\n",
    "\n",
    "    return all_images\n",
    "\n",
    "#detect features using ORB, SIFT, and BRISK\n",
    "def detect_features(detector, images_for_one_video, use_gray=True):\n",
    "    \"\"\"Detects features for a list of images belonging to ONE video.\"\"\"\n",
    "    results = []\n",
    "    for video_folder, filename, img_color, img_gray in images_for_one_video:\n",
    "        img = img_gray if use_gray else img_color\n",
    "        start_time = time.time()\n",
    "        keypoints, descriptors = detector.detectAndCompute(img, None)\n",
    "        time_taken = time.time() - start_time\n",
    "        results.append({\n",
    "            'video_folder': video_folder, # Retain video folder information\n",
    "            'filename': filename,\n",
    "            'image': img_color,\n",
    "            'keypoints': keypoints,\n",
    "            'descriptors': descriptors,\n",
    "            'time': time_taken\n",
    "        })\n",
    "    return results\n",
    "\n",
    "#draw keypoints on images\n",
    "def draw_keypoints(results_for_one_video, alg_name, base_output_dir=\"output_keypoints\"):\n",
    "    \"\"\"Draws keypoints for results from ONE video.\"\"\"\n",
    "    if not results_for_one_video:\n",
    "        return # Nothing to draw\n",
    "\n",
    "    # Create output directory structure: base_output_dir / video_folder\n",
    "    video_folder = results_for_one_video[0]['video_folder']\n",
    "    output_dir = os.path.join(base_output_dir, alg_name, video_folder) # Organize by Algorithm then Video\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(f\"Drawing {alg_name} keypoints for video: {video_folder}\")\n",
    "    for res in results_for_one_video:\n",
    "        img_with_kp = cv2.drawKeypoints(res['image'], res['keypoints'], None, color=(0, 255, 0))\n",
    "        # Save file as original_filename_with_kp.jpg inside the video's output folder\n",
    "        out_filename = os.path.splitext(res['filename'])[0] + \"_with_kp.jpg\"\n",
    "        out_path = os.path.join(output_dir, out_filename)\n",
    "        cv2.imwrite(out_path, img_with_kp)\n",
    "        # print(f\"Saved: {out_path}\") # Suppressed print for cleaner output\n",
    "\n",
    "#analyze results\n",
    "def analysis(results_for_one_video, alg_name, video_folder):\n",
    "    \"\"\"Analyzes results for images from ONE video.\"\"\"\n",
    "    print(f\"\\n - - {alg_name} Analysis for Video: {video_folder} - -\")\n",
    "    total_kps = 0\n",
    "    total_time = 0\n",
    "\n",
    "    if not results_for_one_video:\n",
    "        print(\"No images processed for this video.\")\n",
    "        return\n",
    "\n",
    "    #print summary of results per image in this video\n",
    "    for res in results_for_one_video:\n",
    "        print(f\"{res['filename']}: {len(res['keypoints'])} keypoints in {res['time']:.4f}s\")\n",
    "        total_kps += len(res['keypoints'])\n",
    "        total_time += res['time']\n",
    "\n",
    "    avg_kps = total_kps / len(results_for_one_video)\n",
    "    avg_time = total_time / len(results_for_one_video)\n",
    "    print(f\"Average keypoints per image in {video_folder}: {avg_kps:.2f} | Average Time per image: {avg_time:.4f}s\")\n",
    "\n",
    "\n",
    "# --- Execution for Code Block 2 ---\n",
    "frame_dir = \"frames\"  #path to frames\n",
    "video_folders = sorted([d for d in os.listdir(frame_dir) if os.path.isdir(os.path.join(frame_dir, d))])\n",
    "\n",
    "if not video_folders:\n",
    "    print(f\"No video subfolders found in '{frame_dir}'. Please run Block 1 first.\")\n",
    "else:\n",
    "    print(f\"Processing feature detection for {len(video_folders)} videos...\")\n",
    "\n",
    "    # Initialize detectors once\n",
    "    orb = cv2.ORB_create(nfeatures=1000)\n",
    "    sift = cv2.SIFT_create()\n",
    "    brisk = cv2.BRISK_create()\n",
    "    detectors = {\"ORB\": orb, \"SIFT\": sift, \"BRISK\": brisk}\n",
    "\n",
    "    # Process each video folder separately\n",
    "    for video_folder in video_folders:\n",
    "        video_path = os.path.join(frame_dir, video_folder)\n",
    "        print(f\"\\n--- Processing video: {video_folder} ---\")\n",
    "\n",
    "        # Load images specifically for this video folder\n",
    "        images_for_one_video = []\n",
    "        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "        image_files = sorted([f for f in os.listdir(video_path) if os.path.splitext(f)[1].lower() in image_extensions])\n",
    "\n",
    "        max_images_per_video = 10 # Define the limit per video here as well\n",
    "        for filename in image_files[:max_images_per_video]:\n",
    "             img_path = os.path.join(video_path, filename)\n",
    "             img_color = cv2.imread(img_path)\n",
    "             if img_color is None:\n",
    "                 print(f\"Warning: Couldn't read {img_path}\")\n",
    "                 continue\n",
    "             img_gray = cv2.cvtColor(img_color, cv2.COLOR_BGR2GRAY)\n",
    "             images_for_one_video.append((video_folder, filename, img_color, img_gray)) # Store with video folder name\n",
    "\n",
    "        if not images_for_one_video:\n",
    "            print(f\"No images found in {video_folder}. Skipping feature detection for this video.\")\n",
    "            continue\n",
    "\n",
    "        # Detect features, draw, and analyze for THIS video\n",
    "        for alg_name, detector in detectors.items():\n",
    "            print(f\"\\nRunning {alg_name} detector for {video_folder}...\")\n",
    "            results_for_one_video = detect_features(detector, images_for_one_video)\n",
    "\n",
    "            # Save keypoint visualizations for this video/algorithm if output dir doesn't exist\n",
    "            # Check if the specific video/algorithm output folder exists\n",
    "            video_alg_output_dir = os.path.join(\"output_keypoints\", alg_name, video_folder)\n",
    "            if not os.path.exists(video_alg_output_dir):\n",
    "                 draw_keypoints(results_for_one_video, alg_name, base_output_dir=\"output_keypoints\")\n",
    "            else:\n",
    "                 print(f\"Keypoint visualizations for {alg_name}/{video_folder} already exist, skipping drawing.\")\n",
    "\n",
    "\n",
    "            # Analyze results for this video/algorithm\n",
    "            analysis(results_for_one_video, alg_name, video_folder)\n",
    "\n",
    "    print(\"\\nFeature detection and analysis complete for all videos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd13b070",
   "metadata": {},
   "source": [
    "# 4/5. Feature Matching and Outlier Rejection + Fundamental Matrix Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce227bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing pair: Vid1.mp4/frame_0000.jpg & Vid1.mp4/frame_0001.jpg\n",
      "Total matches before RANSAC: 240\n",
      "Matches after RANSAC: 231\n",
      "\n",
      "Processing pair: Vid1.mp4/frame_0001.jpg & Vid1.mp4/frame_0002.jpg\n",
      "Total matches before RANSAC: 229\n",
      "Matches after RANSAC: 226\n",
      "\n",
      "Processing pair: Vid1.mp4/frame_0002.jpg & Vid1.mp4/frame_0003.jpg\n",
      "Total matches before RANSAC: 246\n",
      "Matches after RANSAC: 237\n",
      "\n",
      "Processing pair: Vid2.mp4/frame_0000.jpg & Vid2.mp4/frame_0001.jpg\n",
      "Total matches before RANSAC: 154\n",
      "Matches after RANSAC: 150\n",
      "\n",
      "Processing pair: Vid2.mp4/frame_0001.jpg & Vid2.mp4/frame_0002.jpg\n",
      "Total matches before RANSAC: 155\n",
      "Matches after RANSAC: 151\n",
      "\n",
      "Processing pair: Vid2.mp4/frame_0002.jpg & Vid2.mp4/frame_0003.jpg\n",
      "Total matches before RANSAC: 158\n",
      "Matches after RANSAC: 152\n",
      "\n",
      "Processing pair: Vid3.mp4/frame_0000.jpg & Vid3.mp4/frame_0001.jpg\n",
      "Total matches before RANSAC: 246\n",
      "Matches after RANSAC: 243\n",
      "\n",
      "Processing pair: Vid3.mp4/frame_0001.jpg & Vid3.mp4/frame_0002.jpg\n",
      "Total matches before RANSAC: 262\n",
      "Matches after RANSAC: 252\n",
      "\n",
      "Processing pair: Vid3.mp4/frame_0002.jpg & Vid3.mp4/frame_0003.jpg\n",
      "Total matches before RANSAC: 257\n",
      "Matches after RANSAC: 248\n",
      "\n",
      "Processing pair: Vid4.mp4/frame_0000.jpg & Vid4.mp4/frame_0001.jpg\n",
      "Total matches before RANSAC: 438\n",
      "Matches after RANSAC: 435\n",
      "\n",
      "Processing pair: Vid4.mp4/frame_0001.jpg & Vid4.mp4/frame_0002.jpg\n",
      "Total matches before RANSAC: 422\n",
      "Matches after RANSAC: 412\n",
      "\n",
      "Processing pair: Vid4.mp4/frame_0002.jpg & Vid4.mp4/frame_0003.jpg\n",
      "Total matches before RANSAC: 421\n",
      "Matches after RANSAC: 413\n",
      "\n",
      "Processing pair: Vid5.mp4/frame_0000.jpg & Vid5.mp4/frame_0001.jpg\n",
      "Total matches before RANSAC: 445\n",
      "Matches after RANSAC: 437\n",
      "\n",
      "Processing pair: Vid5.mp4/frame_0001.jpg & Vid5.mp4/frame_0002.jpg\n",
      "Total matches before RANSAC: 421\n",
      "Matches after RANSAC: 413\n",
      "\n",
      "Processing pair: Vid5.mp4/frame_0002.jpg & Vid5.mp4/frame_0003.jpg\n",
      "Total matches before RANSAC: 420\n",
      "Matches after RANSAC: 413\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#load images from all videos\n",
    "def load_image_pairs(folder_path, step=1, max_pairs=3):\n",
    "    \"\"\"Loads image pairs from a SINGLE video folder.\"\"\"\n",
    "    pairs = []\n",
    "    filenames = sorted([f for f in os.listdir(folder_path) if f.endswith((\".jpg\", \".jpeg\", \".png\"))])\n",
    "    num_pairs = min(len(filenames) - step, max_pairs)\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        img1_filename = filenames[i]\n",
    "        img2_filename = filenames[i + step]\n",
    "        img1_path = os.path.join(folder_path, img1_filename)\n",
    "        img2_path = os.path.join(folder_path, img2_filename)\n",
    "\n",
    "        img1 = cv2.imread(img1_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img2 = cv2.imread(img2_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if img1 is not None and img2 is not None:\n",
    "            # Store full path or relevant info to reconstruct later if needed\n",
    "            # For now, store just the filenames within the video folder context\n",
    "            video_folder_name = os.path.basename(folder_path)\n",
    "            pairs.append((video_folder_name, img1_filename, img2_filename, img1, img2))\n",
    "        else:\n",
    "            print(f\"Warning: Couldn't load pair {img1_path}, {img2_path}\")\n",
    "    return pairs\n",
    "\n",
    "#get feature detector (No change)\n",
    "def get_detector(name=\"SIFT\"):\n",
    "    if name == \"ORB\":\n",
    "        return cv2.ORB_create(nfeatures=1000)\n",
    "    elif name == \"BRISK\":\n",
    "        return cv2.BRISK_create()\n",
    "    else:\n",
    "        return cv2.SIFT_create()\n",
    "\n",
    "#get matcher based on descriptor type (No change)\n",
    "def get_matcher(desc_type):\n",
    "    if desc_type == \"float\":\n",
    "        index_params = dict(algorithm=1, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "        return cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    else:\n",
    "        return cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "#match features between two images (No change)\n",
    "def match_features(detector, img1, img2):\n",
    "    kp1, des1 = detector.detectAndCompute(img1, None)\n",
    "    kp2, des2 = detector.detectAndCompute(img2, None)\n",
    "\n",
    "    if des1 is None or des2 is None:\n",
    "        # print(\"Warning: Descriptors are None.\") # Debug print\n",
    "        return kp1, kp2, []\n",
    "\n",
    "    #check descriptor type\n",
    "    desc_type = \"float\" if des1.dtype == np.float32 else \"binary\"\n",
    "    matcher = get_matcher(desc_type)\n",
    "\n",
    "    matches = []\n",
    "    try:\n",
    "        if desc_type == \"float\":\n",
    "            raw_matches = matcher.knnMatch(des1, des2, k=2)\n",
    "            good_matches = []\n",
    "            for m, n in raw_matches:\n",
    "                if m.distance < 0.75 * n.distance: # Ratio test\n",
    "                    good_matches.append(m)\n",
    "            matches = good_matches\n",
    "        else: # Binary descriptors (ORB, BRISK)\n",
    "            matches = matcher.match(des1, des2)\n",
    "            # Optionally sort for BFMatcher, though crossCheck helps filter already\n",
    "            matches = sorted(matches, key=lambda x: x.distance)\n",
    "    except cv2.error as e:\n",
    "        print(f\"Matching error: {e}\")\n",
    "        matches = [] # Ensure matches is an empty list on error\n",
    "\n",
    "\n",
    "    return kp1, kp2, matches\n",
    "\n",
    "#ransac filter to remove outliers (No change)\n",
    "def ransac_filter(kp1, kp2, matches):\n",
    "    # Need at least 4 points for findFundamentalMat with RANSAC, but 8 is safer\n",
    "    if len(matches) < 8:\n",
    "        # print(f\"Warning: Not enough matches ({len(matches)}) for RANSAC.\") # Debug print\n",
    "        return [], None\n",
    "\n",
    "    #extract location of good matches\n",
    "    try:\n",
    "        pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "        pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        # Use RANSAC to find the Fundamental Matrix and inliers\n",
    "        # Default RANSAC parameters are usually fine\n",
    "        F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.RANSAC)\n",
    "\n",
    "        if mask is None: # Should not happen with RANSAC, but good practice\n",
    "             print(\"Warning: RANSAC mask is None.\")\n",
    "             return [], F\n",
    "\n",
    "        # Apply the mask to filter inliers\n",
    "        inliers = [m for i, m in enumerate(matches) if mask[i]]\n",
    "\n",
    "    except cv2.error as e:\n",
    "        print(f\"RANSAC error: {e}\")\n",
    "        return [], None # Return empty list and None F on error\n",
    "\n",
    "    return inliers, F\n",
    "\n",
    "#draw matches between two images (Modified to include video/pair context)\n",
    "def draw_matches(img1, kp1, img2, kp2, matches, video_folder, img1_filename, img2_filename, match_type, base_out_folder=\"output_matches\"):\n",
    "    \"\"\"Draws matches for a specific pair from a specific video.\"\"\"\n",
    "    # Create output directory structure: base_out_folder / video_folder / match_type\n",
    "    output_dir = os.path.join(base_out_folder, video_folder, match_type.replace(\" \", \"_\").lower())\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Determine if images are grayscale or color for drawing\n",
    "    is_gray = (len(img1.shape) == 2)\n",
    "\n",
    "    # Convert grayscale images to color for drawing if needed\n",
    "    img1_display = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR) if is_gray else img1\n",
    "    img2_display = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR) if is_gray else img2\n",
    "\n",
    "\n",
    "    #draw matches\n",
    "    # Use plot=None to return the image array, then save with cv2.imwrite\n",
    "    matched_img = cv2.drawMatches(img1_display, kp1, img2_display, kp2, matches, None, flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "    # Create a descriptive filename\n",
    "    # Use parts of original filenames to make output filename unique and informative\n",
    "    name1_base = os.path.splitext(img1_filename)[0]\n",
    "    name2_base = os.path.splitext(img2_filename)[0]\n",
    "    out_filename = f\"{name1_base}_vs_{name2_base}_{match_type.replace(' ', '_').lower()}.png\"\n",
    "    out_path = os.path.join(output_dir, out_filename)\n",
    "\n",
    "    # Save the image using cv2.imwrite as it handles color spaces better than plt.savefig in this context\n",
    "    cv2.imwrite(out_path, matched_img)\n",
    "    # print(f\"Saved match visualization: {out_path}\") # Suppressed print\n",
    "\n",
    "# --- Execution for Code Block 3 ---\n",
    "frame_dir = \"frames\"\n",
    "detector_name = \"SIFT\" # Can change detector here\n",
    "detector = get_detector(detector_name)\n",
    "\n",
    "# List to store results for ALL pairs across ALL videos\n",
    "all_pair_processing_results = []\n",
    "\n",
    "video_folders = sorted([d for d in os.listdir(frame_dir) if os.path.isdir(os.path.join(frame_dir, d))])\n",
    "\n",
    "if not video_folders:\n",
    "    print(f\"No video subfolders found in '{frame_dir}'. Please run Block 1 first.\")\n",
    "else:\n",
    "    print(f\"Processing feature matching for {len(video_folders)} videos...\")\n",
    "\n",
    "    # Process each video folder separately\n",
    "    for video_folder in video_folders:\n",
    "        video_path = os.path.join(frame_dir, video_folder)\n",
    "        print(f\"\\n--- Processing matching for video: {video_folder} ---\")\n",
    "\n",
    "        # Load pairs specifically for this video folder\n",
    "        pairs_for_one_video = load_image_pairs(video_path, step=1, max_pairs=3)\n",
    "\n",
    "        if not pairs_for_one_video:\n",
    "            print(f\"No image pairs found in {video_folder}. Skipping matching for this video.\")\n",
    "            continue\n",
    "\n",
    "        # Process each pair within this video\n",
    "        for video_folder_name, fname1, fname2, img1, img2 in pairs_for_one_video:\n",
    "             print(f\"  Processing pair: {fname1} & {fname2}\")\n",
    "\n",
    "             # Perform matching\n",
    "             kp1, kp2, matches = match_features(detector, img1, img2)\n",
    "             print(f\"    Total matches before RANSAC: {len(matches)}\")\n",
    "\n",
    "             # Draw and save matches before RANSAC\n",
    "             # Check if the output directory for this video/match_type exists before drawing\n",
    "             pre_ransac_output_dir = os.path.join(\"output_matches\", video_folder, \"before_ransac\")\n",
    "             if not os.path.exists(pre_ransac_output_dir):\n",
    "                 draw_matches(img1, kp1, img2, kp2, matches, video_folder, fname1, fname2, \"Before RANSAC\")\n",
    "             else:\n",
    "                 # print(\"    Before RANSAC visualizations already exist, skipping drawing.\") # Suppress for cleaner output\n",
    "                 pass\n",
    "\n",
    "\n",
    "             # Perform RANSAC filtering\n",
    "             inliers, F = ransac_filter(kp1, kp2, matches)\n",
    "             print(f\"    Matches after RANSAC: {len(inliers)}\")\n",
    "\n",
    "             # Draw and save matches after RANSAC\n",
    "             # Check if the output directory for this video/match_type exists before drawing\n",
    "             post_ransac_output_dir = os.path.join(\"output_matches\", video_folder, \"after_ransac\")\n",
    "             if not os.path.exists(post_ransac_output_dir):\n",
    "                 draw_matches(img1, kp1, img2, kp2, inliers, video_folder, fname1, fname2, \"After RANSAC\")\n",
    "             else:\n",
    "                 # print(\"    After RANSAC visualizations already exist, skipping drawing.\") # Suppress for cleaner output\n",
    "                 pass\n",
    "\n",
    "             # Store results for this pair\n",
    "             if inliers: # Only store if RANSAC found inliers\n",
    "                 all_pair_processing_results.append({\n",
    "                     'video_folder': video_folder,\n",
    "                     'fname1': fname1,\n",
    "                     'fname2': fname2,\n",
    "                     'kp1': kp1,\n",
    "                     'kp2': kp2,\n",
    "                     'inliers': inliers,\n",
    "                     'F': F\n",
    "                 })\n",
    "             else:\n",
    "                  print(f\"    No inliers found for pair {fname1} & {fname2}. Skipping pose estimation for this pair.\")\n",
    "\n",
    "\n",
    "    print(\"\\nFeature matching and RANSAC complete for all videos.\")\n",
    "    # The list all_pair_processing_results is now available for the next block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4233568b",
   "metadata": {},
   "source": [
    "As required by the assignment, this section explains the underlying calculations involved in computing the fundamental matrix and clarifies the distinction between calibrated and uncalibrated cameras.\n",
    "\n",
    "#### 5a. Understanding Underlying Calculations for Fundamental Matrix Computation\n",
    "\n",
    "When computing the fundamental matrix ($F$) using a function like OpenCV's `cv.findFundamentalMat()`, the process relies on the epipolar constraint, which states that for any pair of corresponding points $p_1$ in the first image and $p_2$ in the second image, the following relationship holds:\n",
    "\n",
    "$$p_2^T F p_1 = 0$$\n",
    "\n",
    "Here, $p_1$ and $p_2$ are the homogeneous coordinates of the points. The fundamental matrix is a $3 \\times 3$ matrix that encapsulates the epipolar geometry between the two uncalibrated views.\n",
    "\n",
    "The fundamental matrix has 9 elements but is determined up to a scale factor, giving 8 degrees of freedom. Additionally, it must satisfy the rank constraint $det(F) = 0$.\n",
    "\n",
    "A common method for estimating the fundamental matrix is the **Eight-Point Algorithm**. This algorithm uses at least 8 pairs of corresponding points to set up a system of linear equations in the elements of $F$. Each corresponding point pair $(p_{1i}, p_{2i})$ contributes one linear equation based on the epipolar constraint. If we stack the elements of $F$ into a vector $f$, we get a system of equations $Af = 0$, where $A$ is a matrix derived from the point coordinates. This system can be solved for $f$ (and thus $F$) using methods like Singular Value Decomposition (SVD).\n",
    "\n",
    "The linear estimate of $F$ from the 8-point algorithm may not satisfy the rank-2 constraint. To enforce this, SVD is typically applied to the estimated $F = U \\Sigma V^T$. We then set the smallest singular value in $\\Sigma$ to zero to produce $\\Sigma'$, resulting in the rank-2 fundamental matrix $F' = U \\Sigma' V^T$.\n",
    "\n",
    "In the presence of outliers (incorrect matches), a robust estimation method like **RANSAC (Random Sample Consensus)** is employed. When you use `cv.findFundamentalMat()` with the `cv2.RANSAC` method, OpenCV is internally implementing a RANSAC-based approach. RANSAC iteratively:\n",
    "1.  Selects a minimal set of 8 point correspondences.\n",
    "2.  Computes a candidate fundamental matrix using these points.\n",
    "3.  Counts how many of the *remaining* points are consistent with this matrix (inliers).\n",
    "4.  Repeats this process many times and chooses the matrix with the largest number of inliers.\n",
    "5.  Optionally, re-estimates the matrix using all identified inliers.\n",
    "\n",
    "This robust estimation is crucial for obtaining an accurate fundamental matrix from noisy real-world data.\n",
    "\n",
    "#### 5b. Difference Between Calibrated and Uncalibrated Cameras\n",
    "\n",
    "The choice of computing the essential matrix ($E$) or the fundamental matrix ($F$) depends on whether your cameras are calibrated.\n",
    "\n",
    "* **Uncalibrated Cameras:**\n",
    "    * **Definition:** Cameras for which the intrinsic parameters (focal length, principal point, distortion coefficients) are **unknown**.\n",
    "    * **Matrix:** The **Fundamental Matrix ($F$)** is used.\n",
    "    * **Coordinates:** $F$ relates corresponding points in **image coordinates (pixels)**.\n",
    "    * **Purpose:** Describes the epipolar geometry (relationship between points and epipolar lines) in the image plane. It can be used to verify point correspondences and find epipolar lines.\n",
    "    * **Limitation:** Without knowing the intrinsic parameters, $F$ does not encode metric information about the scene or the cameras' 3D motion.\n",
    "\n",
    "* **Calibrated Cameras:**\n",
    "    * **Definition:** Cameras for which the intrinsic parameters are **known** (usually determined through a camera calibration procedure).\n",
    "    * **Matrix:** The **Essential Matrix ($E$)** is used.\n",
    "    * **Coordinates:** $E$ relates corresponding points in **normalized image coordinates**. These coordinates are obtained by removing the effect of intrinsic parameters (e.g., by multiplying image coordinates by the inverse of the camera intrinsic matrix, $K^{-1}$).\n",
    "    * **Relationship:** The essential matrix is related to the fundamental matrix by the camera intrinsic matrices $K_1$ and $K_2$: $E = K_2^T F K_1$.\n",
    "    * **Purpose:** Describes the epipolar geometry in a normalized coordinate system and crucially encodes the **relative rotation and translation** between the two camera views.\n",
    "    * **Advantage:** Knowing the intrinsic parameters and having the essential matrix allows for the recovery of the camera's relative 3D pose and enables metric 3D reconstruction of the scene.\n",
    "\n",
    "In the context of this assignment, the videos were taken using an uncalibrated mobile phone camera. Therefore the fundamental matrix was computed and not the essential matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b83e14",
   "metadata": {},
   "source": [
    "# 6. Camera Position Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cef311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Placeholder Camera Matrix:\n",
      "[[4.50e+03 0.00e+00 2.39e+02]\n",
      " [0.00e+00 4.50e+03 4.25e+02]\n",
      " [0.00e+00 0.00e+00 1.00e+00]]\n",
      "\n",
      "--- Camera Pose Estimation Results ---\n",
      "Number of points successfully reconstructed in front of both cameras: 388\n",
      "\n",
      "Estimated Rotation Matrix (R):\n",
      "[[ 0.851938    0.17702365 -0.49281261]\n",
      " [ 0.18533347  0.77827135  0.59995434]\n",
      " [ 0.48974804 -0.60245858  0.63023053]]\n",
      "\n",
      "Estimated Translation Vector (t):\n",
      "[[0.77176872]\n",
      " [0.63589578]\n",
      " [0.00309864]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Placeholder Camera Matrix ---\n",
    "pixel_focal_length = 4500\n",
    "image_width = 478\n",
    "image_height = 850\n",
    "\n",
    "# Principal point assumed at the image center\n",
    "principal_point_x = image_width / 2\n",
    "principal_point_y = image_height / 2\n",
    "\n",
    "camera_matrix = np.array([[pixel_focal_length, 0, principal_point_x], [0, pixel_focal_length, principal_point_y], [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "print(\"Using Placeholder Camera Matrix:\")\n",
    "print(camera_matrix)\n",
    "\n",
    "# --- Execution for Code Block 4 ---\n",
    "# This block assumes 'all_pair_processing_results' list is available from Block 3\n",
    "\n",
    "if 'all_pair_processing_results' not in locals() or not all_pair_processing_results:\n",
    "    print(\"\\nNo pair processing results found from Block 3.\")\n",
    "    print(\"Please run Block 3 first and ensure it generated results with inliers.\")\n",
    "else:\n",
    "    print(f\"\\nProcessing pose estimation for {len(all_pair_processing_results)} pairs...\")\n",
    "\n",
    "    # Iterate through the results collected from each pair in Block 3\n",
    "    for pair_result in all_pair_processing_results:\n",
    "        video_folder = pair_result['video_folder']\n",
    "        fname1 = pair_result['fname1']\n",
    "        fname2 = pair_result['fname2']\n",
    "        kp1 = pair_result['kp1']\n",
    "        kp2 = pair_result['kp2']\n",
    "        inliers = pair_result['inliers']\n",
    "        F = pair_result['F'] # Fundamental Matrix from RANSAC\n",
    "\n",
    "        print(f\"\\n--- Estimating Pose for Video: {video_folder}, Pair: {fname1} & {fname2} ---\")\n",
    "\n",
    "        if len(inliers) < 8: # Need at least 8 points for Fundamental Matrix based methods, though recoverPose can use fewer with E\n",
    "            print(f\"  Not enough inliers ({len(inliers)}) for reliable pose estimation. Skipping this pair.\")\n",
    "            continue\n",
    "\n",
    "        # Extract points from inlier matches\n",
    "        # Reshape is needed for cv2.recoverPose format\n",
    "        try:\n",
    "            pts1 = np.float32([kp1[m.queryIdx].pt for m in inliers]).reshape(-1, 1, 2)\n",
    "            pts2 = np.float32([kp2[m.trainIdx].pt for m in inliers]).reshape(-1, 1, 2)\n",
    "\n",
    "            # Estimate camera pose using recoverPose\n",
    "            # recoverPose requires the Essential Matrix, which can be derived from F and K\n",
    "            # E = K.T * F * K\n",
    "            # However, cv2.recoverPose can also take F and K separately to compute E internally.\n",
    "            # It returns the number of cheiral inliers (points in front of both cameras),\n",
    "            # Rotation (R), Translation (t), and a mask indicating cheiral points.\n",
    "\n",
    "            # Note: Using F with K will provide scaled translation.\n",
    "            ret, R, t, mask = cv2.recoverPose(F, pts1, pts2, camera_matrix)\n",
    "\n",
    "            print(f\"  Number of points successfully reconstructed in front of both cameras: {ret}\")\n",
    "            print(\"\\n  Estimated Rotation Matrix (R):\")\n",
    "            print(R)\n",
    "            print(\"\\n  Estimated Translation Vector (t):\")\n",
    "            print(t.flatten()) # Print translation vector flattened\n",
    "\n",
    "        except cv2.error as e:\n",
    "             print(f\"  Error during cv2.recoverPose for pair {fname1} & {fname2}: {e}\")\n",
    "        except Exception as e:\n",
    "             print(f\"  An unexpected error occurred during pose estimation for pair {fname1} & {fname2}: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\nPose estimation complete for all processed pairs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
